{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce08bfc-1f6d-4787-b7e4-bbef0c4db778",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Worteinbettung und Sprachmodelle\n",
    "Allen Vektorraum-Modellen ist gemeinsam, dass sie weder Semantik von Wörtern noch Ähnlichkeit erfassen. In diesem Abschnitt werden Worteinbettungen bzw. Embeddings verwendet, um die Bedeutung von Wörtern auf Basis des gemeinsamen Auftretens mit anderen Wörtern zu kodieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ea6ac-60ee-4f6e-aef1-cd2ba0524610",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bda73-c7bf-4e40-80c8-262bdc199e0b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Inhaltsverzeichnis\n",
    "- [Word2Vec](#Word2Vec)\n",
    "- [Implementierung](#Implementierung)\n",
    "- [Zusammenfassung](#Zusammenfassung)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bce29-ec67-4c5c-aecd-d89ffca7041d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Word2Vec\n",
    "Word2Vec ist ein Algorithmus zur Darstellung von Wörtern als Vektoren. Die Grundidee hinter Word2Vec ist, dass Wörter, die in ähnlichen Kontexten verwendet werden, ähnliche Vektoren haben sollten. Die Bedeutung eines Wortes wird damit mit Machine Learning aus den anderen Worten gelernt, mit denen es häufig zusammen auftritt, sodass ein großer Korpus notwendig ist. Die entstehenden Vektoren können dann verwendet werden, um semantische Beziehungen zwischen Wörtern zu erfassen.\n",
    "\n",
    "Es gibt zwei Hauptarchitekturen, die von Word2Vec verwendet werden, um diese Vektoren zu lernen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eeeaa0-887a-4121-bd08-fb4d8a8c62cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Continuous Bag of Words (CBOW)\n",
    "Bei dieser Methode wird das Modell darauf trainiert, ein Wort (das Zielwort) basierend auf dem Kontext (den umgebenden Wörtern) vorherzusagen. Es nimmt eine bestimmte Anzahl von Wörtern vor und nach dem Zielwort und versucht, das Zielwort aus diesen Kontextwörtern zu rekonstruieren.\n",
    "\n",
    "**Beispiel:**\n",
    "`Das Auto fährt auf der Straße.`\n",
    "\n",
    "Wenn das aktuelle Zielwort `fährt` ist, dann wird das Modell darauf trainiert dieses Zielwort auf Basis des Kontexts `Das`, `Auto` und `auf`, `der`, `Straße` vorherzusagen. (Die Fenstergröße lässt sich dabei beeinflussen, sodass kleinere oder größere Teile des Kontexts einfließen.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6a273-2a21-4288-b37a-6f040a984938",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Skip-Gram\n",
    "Im Gegensatz zum CBOW-Modell geht es beim Skip-Gram-Modell darum, ein Zielwort zu verwenden, um den Kontext vorherzusagen. Es wird also ein Wort (das Zielwort) genommen und versucht, die umgebenden Kontextwörter zu bestimmen.\n",
    "\n",
    "**Beispiel**: `Das Auto fährt auf der Straße.`\n",
    "\n",
    "Wenn das aktuelle Zielwort erneut `fährt` ist, dann wird das Modell darauf trainiert, die in der Umgebung befindlichen Kontextwörter `Das`, `Auto`, `auf`, `der` und `Straße` zu rekonstruieren. (Erneut lässt sich die Fenstergröße anpassen.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bfd9c-eb63-4f3f-8485-f537dfea4468",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Umsetzung\n",
    "Zur Umsetzung wird ein neuronales Netz trainiert, das in der Regel mit einer versteckten Schicht auskommt. Die Ein- und Ausgabeschichten hängen dann von der verwendeten Architektur ab.\n",
    "\n",
    "| Architektur | Eingabe                                   | Ausgabe                                               |\n",
    "| ----------- | ----------------------------------------- | ----------------------------------------------------- |\n",
    "| CBOW        | 1 One-Hot-kodierter Vektor des Zielwortes | n Wahrscheinlichkeitsvektoren (*SoftMax*) für Kontext |\n",
    "| Skip-Gram   | n One-Hot-kodierte Vektoren des Kontexts  | 1 Wahrscheinlichkeitsvektor (*SoftMax*) für Zielwort  |\n",
    "\n",
    "Interessant ist aber in beiden Fällen die versteckte Schicht. Diese ist eine lineare Transformation des oder der Eingabevektoren und führt am Ende zur gewünschten Vorhersage. Eingabewörter, die eine ähnliche semantische Bedeutung haben und damit in einem ähnlichen Kontext verwendet werden, müssen also nach dem Training zwangsweise eine ähnliche Zwischenrepräsentation erzeugen.\n",
    "\n",
    "Wird diese Zwischenrepräsentation - die Ausgabe der versteckten Schicht nach einer Aktivierung - als Vektor interpretiert, werden in ähnlichem Kontext verwendete Wörter auf ähnliche Vektoren abgebildet, die als Worteinbettungen (*Word Embeddings*) bezeichnet werden. (Die Größe der versteckten Schicht bestimmt somit direkt die Größe des Vektors und damit die Dimension des Vektorraums.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee044d2-3bf2-4de0-8dca-111e46ef8e25",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Implementierung\n",
    "Definition und Training neuronaler Netze werden an anderer Stelle gelehrt. Stattdessen soll eine fertige Implementierung aus dem Paket [Gensim](https://pypi.org/project/gensim/) an folgendem Beispieltext veranschaulicht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b6e92f-5c45-48d2-ba91-96efd0969257",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"der\", \"rechner\", \"spinnt\", \"schon\", \"wieder\"],\n",
    "    [\"ich\", \"habe\", \"mir\", \"einen\", \"neuen\", \"computer\", \"gekauft\"],\n",
    "    [\"der\", \"pc\", \"vom\", \"arbeitskollegen\", \"ist\", \"besser\", \"als\", \"meiner\"],\n",
    "    [\"der\", \"laptop\", \"funktioniert\", \"nicht\", \"richtig\"],\n",
    "    [\"ich\", \"habe\", \"mir\", \"ein\", \"neues\", \"smartphone\", \"besorgt\"],\n",
    "    [\"der\", \"drucker\", \"ist\", \"kaputt\", \"gegangen\"],\n",
    "    [\"mein\", \"tablet\", \"ist\", \"sehr\", \"langsam\", \"geworden\"],\n",
    "    [\"der\", \"monitor\", \"von\", \"meinem\", \"freund\", \"ist\", \"größer\", \"als\", \"meiner\"],\n",
    "    [\"ich\", \"muss\", \"mein\", \"operating\", \"system\", \"aktualisieren\"],\n",
    "    [\"mein\", \"speicher\", \"ist\", \"fast\", \"voll\"],\n",
    "    [\"die\", \"kamera\", \"auf\", \"dem\", \"neuen\", \"handy\", \"ist\", \"sehr\", \"gut\"],\n",
    "    [\"der\", \"pc\", \"von\", \"meinem\", \"bruder\", \"läuft\", \"schneller\", \"als\", \"mein\", \"computer\"],\n",
    "    [\"ich\", \"habe\", \"mein\", \"wifi\", \"gerade\", \"auf\", \"ein\", \"besseres\", \"netzwerk\", \"umgestellt\"],\n",
    "    [\"mein\", \"laptop\", \"hat\", \"schon\", \"sehr\", \"viele\", \"jahre\", \"auf\", \"dem\", \"buckel\"],\n",
    "    [\"die\", \"neue\", \"software\", \"auf\", \"meinem\", \"computer\", \"läuft\", \"flüssiger\"],\n",
    "    [\"der\", \"akku\", \"meines\", \"laptops\", \"hält\", \"nicht\", \"mehr\", \"lange\"],\n",
    "    [\"der\", \"speicher\", \"meines\", \"smartphones\", \"reicht\", \"nicht\", \"mehr\", \"für\", \"alle\", \"meine\", \"bilder\"],\n",
    "    [\"mein\", \"freund\", \"hat\", \"sich\", \"eine\", \"neue\", \"grafikkarte\", \"gekauft\"],\n",
    "    [\"die\", \"website\", \"lässt\", \"sich\", \"auf\", \"meinem\", \"browser\", \"nicht\", \"laden\"],\n",
    "    [\"ich\", \"habe\", \"meinen\", \"pc\", \"aufgerüstet\", \"mit\", \"mehr\", \"ram\"],\n",
    "    [\"mein\", \"laptop\", \"hat\", \"einen\", \"defekten\", \"bildschirm\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7482eab-eb06-4635-862f-ddf6fab03717",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Das Training wird durch die Klasse `Word2Vec` abstrahiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e261c3e5-2fd9-48a4-989c-437a01712736",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences, vector_size=4, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539d583-56d9-44ea-b752-d8eadfb85c9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Über das Attribut `wv` (**Word Vectors**) lassen sich die Vektoren abfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7c141-a630-41f6-a855-84fb25e3f755",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47f41b-c825-4229-8732-230b8121ae2d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model.wv['smartphone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492c2cc-de0e-41f5-b1f7-d8eef628f1d1",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model.wv['arbeitskollegen']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eafdd5f-c752-418c-9944-3002929dc06a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Außerdem ist die Suche nach ähnlichen Vektoren und damit ähnlichen Wörtern bereits integriert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18ae6d-61ac-445f-90fd-5db39c597dee",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('computer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab616b-720d-4513-8aaf-98619bf9da0e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Zusammenfassung\n",
    "Worteinbettungen beziehen im Gegensatz zu den zuvor gezeigten Methoden auch den Kontext mit ein und schaffen es so, ähnlich verwendete Wörter zu ähnlichen Vektoren zu transformieren. Auch hier sollten Sie den Code erneut mit einem größeren Korpus und ggf. einer ausgefeilteren Vorverarbeitung probieren. (Übliche Vektorgrößen liegen im Bereich von $100$ bis $300$. Manchmal verbessern auch Größen bis zu $1.000$ noch die Genauigkeit spürbar.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
