{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce08bfc-1f6d-4787-b7e4-bbef0c4db778",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Anwendungsfälle\n",
    "In diesem Abschnitt werden einige praktische Anwendungsfälle der Textanalyse bearbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ea6ac-60ee-4f6e-aef1-cd2ba0524610",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import pke\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "stopwords_and_punctuation = set((*stopwords.words('english'), ',', '.', ';', ':', '(', ')', '!', '?', \"'\", '’'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bda73-c7bf-4e40-80c8-262bdc199e0b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Inhaltsverzeichnis\n",
    "- [Sentimentanalyse](#Sentimentanalyse)\n",
    "- [Keyphrase- und Keyword-Extraktion](#Keyphrase--und-Keyword-Extraktion)\n",
    "- [Topic Modeling](#Topic-Modeling)\n",
    "- [Zusammenfassung](#Zusammenfassung)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a284b5c-83e3-499a-b290-999824888b66",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Sentimentanalyse\n",
    "Sentimentanalyse beschreibt die Extraktion von Empfindungen und Gefühlen. Abstrakter formuliert soll einem Text eine Stimmung zugeordnet werden, welche die Intention in positiv, negativ oder neutral kategorisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed25617-c4b8-4d62-b905-3dd6d5f4e4af",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    'I really like the new design of your website!',\n",
    "    'I was on hold for 30 minutes. Do I have to say more?',\n",
    "    'At least the meal was still hot.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051bccf-4b3c-47ee-b31c-7e9a65d1e9c0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Ein naiver Ansatz versieht jedes (relevante) Token mit einem Wert. Dabei steht $-1$ für negativ, $0$ für neutral und $1$ für positiv. Wird für jedes Token der Wert aus dem Sentiment-Wörterbuch entnommen und für eine Aussage aufsummiert, ergibt sich zumindest eine grobe Richtung der Intention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bee52-7460-4fb0-a6f4-8083e73e993e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "sentiment_dict = {\n",
    "    'like': 1,\n",
    "    'new': 1,\n",
    "    'hold': -1,\n",
    "    'least': -1,\n",
    "    'hot': 1\n",
    "}\n",
    "\n",
    "for statement in reviews:\n",
    "    tokens = [token for token in word_tokenize(statement) if token not in stopwords_and_punctuation]\n",
    "    sentiment = sum(sentiment_dict.get(token, 0) for token in tokens)\n",
    "\n",
    "    print(f'{sentiment=:>2} {statement=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38041cec-dfc6-43bf-adb9-65239bcd2753",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Diese Methode hat jedoch mehrere Nachteile. Zuerst muss natürlich das Wörterbuch erstellt werden, das zumindest teilweise von der Situation abhängig ist, in dem die Statements abgegeben werden. Außerdem ist es mit lexikonbasierten Ansätzen in der Regel nicht möglich Mehrdeutigkeiten, Negationen oder auch Ironie korrekt zu verarbeiten. Ebenso sind Wörter häufig gar nicht alleinstehend bewertbar: Eine kurze Wartezeit ist etwas Gutes, obwohl die Wartezeit selbst nichts erstrebenswertes ist. Glück ist ebenfalls etwas Gutes, aber wenn es nur kurz währte, ist die Aussage wohl eher negativ gemeint. Der Kontext ist also entscheidend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d29d66-c2b0-46c6-a2f9-d2796e2556e9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Mit Hilfe vortrainierter Modelle, die beispielsweise auf der Transformer-Architektur basieren, lässt sich der Kontext eines Worte viel besser bewerten. Das Training wie auch die Bewertung ist aber natürlich deutlich rechenintensiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acac43e-77aa-4cfa-b952-75807e06f43c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline('sentiment-analysis',\n",
    "                              model='distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                              device='cpu')\n",
    "sentiment_pipeline(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fdcaa1-09fe-49df-a59e-306fb270374d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Die Ergebnisse schwanken in Abhängigkeit des verwendeten Modells. DistilBERT scheint beispielsweise nicht korrekt mit der im dritten Statement enthaltenen, impliziten Aussage umgehen zu können. In der Regel erzeugen neuere Modell oder solche mit mehr Parametern bessere Ergebnisse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d866b4-52d0-4d86-9f4a-ad066e5de04a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Keyphrase- und Keyword-Extraktion\n",
    "Keyphrase-Extraktion ordnet einem Text die in ihm enthaltenen, wichtigsten Terme zu. Verwendet werden die Keyphrases dann zum Beispiel, um eine Zusammenfassung oder eine thematische Beschreibung zu erzeugen. (Die Keyphrase-Extraktion sollte nicht mit Keyword-Assignment verwechselt werden. Letztere extrahiert keine Schlüsselworte, sondern weist eines oder mehrere aus einer vorher bestimmten Menge zu und ist damit eine Klassifikationsaufgabe.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c420e9d-eb7e-446c-9133-81326739fb92",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In einem vorangegangenen Abschnitt haben wir bereits festgestellt, dass TF-IDF den einzelnen Tokens einen Wert zuordnet, welcher die Relevanz widerspiegelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f220697-203f-4736-8159-be236c074086",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words=list(stopwords_and_punctuation))\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "counts = count.fit_transform(reviews)\n",
    "tfidf = tfidf.fit_transform(counts)\n",
    "tokens = np.array(count.get_feature_names_out())\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1043d-37fc-4a68-a68e-01a014ea2374",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Sortieren wir den TF-IDF Vektor für ein Dokument absteigend, dann erhalten wir damit die relevantesten Tokens für eben dieses Dokument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23aa402-9a80-4473-aeb7-e3e4e75bca10",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "top_n = 2\n",
    "\n",
    "for i, review in enumerate(reviews):\n",
    "    review_tfidf = tfidf[i].toarray().flatten()\n",
    "    sorted_indices = np.argsort(review_tfidf)[::-1]\n",
    "\n",
    "    print(review)\n",
    "    print([(tokens[index], review_tfidf[index]) for index in sorted_indices[:top_n]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9a8b7-fb1e-420f-948b-91e179e772f9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Wie üblich gibt es eine Python-Bibliothek, die komplexere Ansätze implementiert. [pke](https://github.com/boudinfl/pke) verwendet SpaCy und stellt [eine ganze Reihe](https://github.com/boudinfl/pke?tab=readme-ov-file#implemented-models) von Methoden zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7daad3-2003-4889-8a71-7c2c520dce90",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "top_n = 2\n",
    "\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "for review in reviews:\n",
    "    extractor.load_document(input=review, language='en')\n",
    "    extractor.candidate_selection()\n",
    "    extractor.candidate_weighting()\n",
    "\n",
    "    print(review)\n",
    "    print(extractor.get_n_best(n=top_n))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbb1d5-a67a-44ae-914d-b5574e5da19e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Topic Modeling\n",
    "Topic Modeling bestimmt Themen in einem Text in Abhängigkeit der Häufung von Wörtern. Dabei wird aber kein abstraktes Oberthema zugeordnet, sondern eine Gruppe von Wörtern aus dem Text als Thema bestimmt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722b99d-9656-4ed3-99c6-6399af91a585",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "*Latent Dirichlet Allocation* ist ein Verfahren, das jedes Dokument als eine Ansammlung von Wörtern annimmt. Es wird davon ausgegangen, dass jedes Dokument aus mehreren *verborgenen* Themen (latent topics) besteht, die sich jeweils durch eine Menge von Wörtern beschreiben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d430084-5d53-4bd3-9cd4-1133730d60c8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "review_tokens = [\n",
    "    [token for token in word_tokenize(review) if token not in stopwords_and_punctuation]\n",
    "    for review in reviews\n",
    "]\n",
    "\n",
    "id2word = corpora.Dictionary(review_tokens)\n",
    "corpus = [id2word.doc2bow(r) for r in review_tokens]\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=5)\n",
    "\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e91ca-e089-4c3d-9593-51ccbdaa37e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Das Ergebnis enthält die vorgegebene Anzahl an Themen, die jeweils aus einer Menge von Termen besteht. Den Thermen wird jeweisl eine Wahrscheinlichkeit zugeordnet, dass sie zu diesem bestimmten Thema gehören."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ecef1c-7050-4a05-a3c7-5ab2fe32c594",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Zusammenfassung\n",
    "In diesem Abschnitt wurden einige praktische Ansätze der Textverarbeitung vorgestellt. Sie sollten diese noch einmal auf Texte mit größerem Umfang anwenden, um aussagekräftigere Ergebnisse zu erhalten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
