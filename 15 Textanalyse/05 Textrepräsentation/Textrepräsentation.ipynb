{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce08bfc-1f6d-4787-b7e4-bbef0c4db778",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Textrepräsentation\n",
    "Um Texte effizient mit Methoden der Data Science verarbeiten zu können, müssen diese in einen einheitlichen, numerisch berechenbaren Raum konvertiert werden. In diesem Abschnitt werden dazu zwei Methoden präsentiert, die Texte in Vektoren konvertieren, um diese anschließend durch Ähnlichkeitsmaße innerhalb eines Vektorraums zu vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ea6ac-60ee-4f6e-aef1-cd2ba0524610",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_and_punctuation = set((*stopwords.words('german'), ',', '.', ';', ':', '(', ')', '!', '?', \"'\", '’'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bda73-c7bf-4e40-80c8-262bdc199e0b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Inhaltsverzeichnis\n",
    "- [Beispieltexte](#Beispieltexte)\n",
    "- [Bag-of-Words-Modell](#Bag-of-Words-Modell)\n",
    "- [TF-IDF Modell](#TF-IDF-Modell)\n",
    "- [Cosinus-Ähnlichkeit](#Cosinus-Ähnlichkeit)\n",
    "- [Zusammenfassung](#Zusammenfassung)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387913e-29e1-4714-b1ae-8b0720db5a86",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Beispieltexte\n",
    "Im Folgenden verwenden wir einfache Beispieltexte zur Veranschaulichung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563d407-cf63-4d6a-8910-c28f2a6ef67b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "example_texts = [\n",
    "    'Ich liebe Hunde, denn Hunde sind die tollsten Tiere.',\n",
    "    'Ich liebe Katzen. Katzen sind einfach unabhängig und mutig.',\n",
    "    'Ich liebe Katzen und Hunde.',\n",
    "    'Ich liebe Delphine. Im Wasser lebende Säugetiere sind mythisch.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c5d6b-5c72-4017-a1ab-9853f764b874",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Bag-of-Words-Modell\n",
    "Das Bag-of-Words-Modell basiert auf der Idee, dass der Inhalt eines Textes durch die Häufigkeit der Wörter im Text beschrieben werden kann. Beim Erstellen der Repräsentation eines Textes wird daher auch die Reihenfolge und Struktur der Wörter ignoriert und tatsächlich nur die Anzahl gezählt, wie oft jedes Wort im Dokument vorkommt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a184c30-fd5b-43e5-84c9-b87d9d2e185c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Das Erstellen der Vektoren funktioniert nun in zwei Schritten. Zuerst wird ein sogenanntes *Vokabular* erstellt, indem jedem einzelnen Wort ein Position für den späteren Vektor zugeteilt wird. Das Vokabular für drei Sätze \"Ich liebe Hunde.\", \"Ich liebe Katzen.\" und \"Ich hasse Delphine.\" wäre nach Entfernung von Stoppwörtern beispielsweise\n",
    "\n",
    "```python\n",
    "['liebe', 'Hunde', 'Katzen', 'hasse', 'Delphine']\n",
    "```\n",
    "\n",
    "Die Reihenfolge an sich ist nicht entscheidend. Relevant ist nur, dass sie im weiteren Verlauf konstant bleibt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c9799-d148-484e-b737-b197d939652a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Für die Implementierung in Python wird ein Dictionary verwendet, um später anhand eines Wortes schnell den Index zu finden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdb68f-2b15-45af-a5de-bad3e91fc73d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def bow_vocabulary(texts):\n",
    "    voc = {}\n",
    "\n",
    "    for text in texts:\n",
    "        for token in word_tokenize(text.casefold()):\n",
    "            if token not in stopwords_and_punctuation and token not in voc:\n",
    "                voc[token] = len(voc)\n",
    "\n",
    "    return voc\n",
    "\n",
    "\n",
    "example_vocabulary = bow_vocabulary(example_texts)\n",
    "example_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e1c2c-bf3f-4f74-a362-eb1ddc8f3d42",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Im zweiten Schritt wird jedem Text ein Vektor zugeordnet. Dieser Vektor basiert auf dem Vokabular und enthält die Häufigkeit des dieser Stelle des Vektors zugeordneten Wortes im Text.\n",
    "\n",
    "Die oben genannten Textbeispiele führen daher zu folgenden Vektoren:\n",
    "- \"Ich liebe Hunde.\" $\\rightarrow$ `[1, 1, 0, 0, 0]`\n",
    "- \"Ich liebe Katzen.\" $\\rightarrow$ `[1, 0, 1, 0, 0]`\n",
    "- \"Ich hasse Delphine.\" $\\rightarrow$ `[0, 0, 0, 1, 0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754a2f8-e793-4c14-91c7-16147f8b9fa2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Mit Hilfe des erzeugten Dictionaries lassen sich solche Vektoren nun auch einfach in Python bilden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811aadf6-4057-4a61-8e11-ee3c2c35d6ce",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def bow_vector(vocabulary, text):\n",
    "    vector = [0 for _ in vocabulary]\n",
    "\n",
    "    for token in word_tokenize(text.casefold()):\n",
    "        if token in vocabulary:\n",
    "            vector[vocabulary[token]] += 1\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "example_bow_vectors = [bow_vector(example_vocabulary, text) for text in example_texts]\n",
    "example_bow_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e2c667-38cb-4299-b130-e09cfe72a33a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Wenn für jeden Text bzw. für jedes Dokument ein solcher Vektor erzeugt wurde, lassen sich diese durch Einsatz einer geeigneten Ähnlichkeitsfunktion vergleichen. Die folgende Zelle verwendet dafür die Cosinus-Ähnlichkeit, auf die wir später noch einmal zurückkommen. Für den Moment reicht es zu wissen, dass ein Wert zwischen $0$ und $1$ erzeugt wird, wobei $0$ für keine Ähnlichkeit und $1$ für maximale Ähnlichkeit steht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8a0e8-e9d1-4259-bdd8-4cb589cd26f5",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "for text1, vector1 in zip(example_texts, example_bow_vectors):\n",
    "    for text2, vector2 in zip(example_texts, example_bow_vectors):\n",
    "        if text1 >= text2:\n",
    "            continue\n",
    "\n",
    "        np_vector1 = np.array(vector1)\n",
    "        np_vector2 = np.array(vector2)\n",
    "\n",
    "        print(text1)\n",
    "        print(text2)\n",
    "        print(np.dot(np_vector1, np_vector2) / (np.linalg.norm(np_vector1) * np.linalg.norm(np_vector2)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28314815-560d-462d-9e13-5954e6a4a929",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Alternativ lässt sich das BoW-Modell auch in einer binären Form umsetzen. Dafür wird entweder $1$ im Vektor gespeichert, wenn der Term *mindestens* einmal vorkommt, oder $0$ andernfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc782cd8-205d-4d09-a442-3ef7fd9aaf1e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## TF-IDF Modell\n",
    "Im Bag-of-Words-Modell wird die absolute Häufigkeit aller Wörter betrachtet und damit auch jedes Wort gleich gewichtet. Es ist aber davon auszugehen, dass ein Wort, das zehn Mal vorkommt, nicht zehn Mal so relevant innerhalb eines Textes ist wie ein Wort, das nur ein einziges Mal vorkommt. Zudem sollten Worte, die in relativ vielen der Texte vorkommen, weniger gewichtet werden, da sie keine Information repräsentieren, welche die Texte voneinander unterscheidbar macht.\n",
    "\n",
    "Das TF-IDF Modell bezieht daher auch Dokument- und Korpusstatistiken mit ein. Dazu führen wir mehrere Häufigkeiten ein, die wir anschließend miteinander ins Verhältnis setzen:\n",
    "- **Term Frequency** (Termhäufigkeit) $tf_{t,d}$ bezeichnet die Häufigkeit des Auftretens des Terms (Wortes) $t$ im Dokument (Text) $d$.\n",
    "- **Document Frequency** (Dokumenthäufigkeit) $df_t$ bezeichnet die Anzahl der Dokumente, die den Term $t$ enthalten.\n",
    "- **Inverse Document Frequence** (Inverse Dokumentenhäufigkeit) $idf_t$ bildet sich für jeden einzelnen Term $t$ als $$ idf_t = log\\left( \\frac{\\left| D \\right|}{df_t} \\right) $$\n",
    "\n",
    "Die TF-IDF Gewichtung $w_{t,d}$ für einen Term $t$ innerhalb eines Dokuments $d$ berechnet sich dann aus der Häufigkeit des Terms im Dokument und der inversen Dokumentenhäufigkeit des Terms: $$ w_{t,d} = tf_{t,d} * idf_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77f5f5-af3e-4755-9e44-f6a87073a0e4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Aber ineffiziente, aber einfach verständliche Umsetzung in Python könnte dann zum Beispiel wie folgt aussehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5067c-5574-4273-9fef-ddf61c47190b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def tf(term, document):\n",
    "    count = 0\n",
    "\n",
    "    for token in word_tokenize(document.casefold()):\n",
    "            if token not in stopwords_and_punctuation and term == token:\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def df(term, documents):\n",
    "    count = 0\n",
    "\n",
    "    for document in documents:\n",
    "        if tf(term, document) > 0:\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def idf(term, documents):\n",
    "    return math.log(len(documents) / df(term, documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe14f53-5b4f-4c69-95f4-86b660803665",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def w(term, document, documents):\n",
    "    return tf(term, document) * idf(term, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20682e-42a3-45cb-bb8f-79b5045c0b16",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def tfidf_vector(vocabulary, document, documents):\n",
    "    return [w(term, document, documents) for term in vocabulary]\n",
    "\n",
    "example_tfidf_vectors = [tfidf_vector(example_vocabulary, text, example_texts) for text in example_texts]\n",
    "example_tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7253f-148c-432a-a4fd-c704bf600c16",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Auch diese Vektoren lassen sich zum Beispiel mithilfe der Cosinus-Ähnlichkeit vergleichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e8aac-bed5-4768-8ac1-6f9ea95091eb",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "for text1, vector1 in zip(example_texts, example_tfidf_vectors):\n",
    "    for text2, vector2 in zip(example_texts, example_tfidf_vectors):\n",
    "        if text1 >= text2:\n",
    "            continue\n",
    "\n",
    "        np_vector1 = np.array(vector1)\n",
    "        np_vector2 = np.array(vector2)\n",
    "\n",
    "        print(text1)\n",
    "        print(text2)\n",
    "        print(np.dot(np_vector1, np_vector2) / (np.linalg.norm(np_vector1) * np.linalg.norm(np_vector2)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db682b74-7d92-4588-b536-86fb1bc79ed9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Es fällt dabei auf, dass die Ähnlichkeiten auf $0$ sinken, wenn von verschiedenen Tieren gesprochen wird. Der übereinstimmende Term *liebe* wird vom BoW-Modell regulär gezählt, während das TF-IDF Modell diesem Term keine Bedeutung zuordnet, da er in jedem einzelnen Dokument vorkommt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ec6cc-2530-432a-80c6-f372d7f75873",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Cosinus-Ähnlichkeit\n",
    "Die Länge eines Textes hat Einfluss auf den gebildeten Vektor. Beim BoW-Modell steigt mit der Anzahl der Terme auch die Summe über alle Elemente des Vektors, während beim TF-IDF Modell die Anzahl der Elemente abnimmt, die $0$ sind. Ein Vergleich zweier Vektoren sollte daher unabhängig von der Länge des Vektors sein.\n",
    "\n",
    "Die Cosinus-Ähnlichkeit ist ein Maß für die Ähnlichkeit zwischen zwei Vektoren im n-dimensionalen Raum, das häufig in der Textanalyse und im maschinellen Lernen verwendet wird. Sie basiert auf dem Cosinus des Winkels zwischen den beiden Vektoren und gibt an, wie ähnlich diese zueinander ausgerichtet sind. Ein Wert von $1$ bedeutet, dass die Vektoren einander **überlagern** (also exakt in die gleiche Richtung weisen), während ein Wert von $0$ darauf schließen lässt, dass sie orthogonal zueinander stehen und somit maximal unähnlich sind. Die Länge des Vektors wird somit gar nicht einbezogen.\n",
    "\n",
    "Der Cosinus des Winkels $\\theta$ zwischen zwei Vektoren $v_1$ und $v_2$ berechnet sich wie folgt. Dabei bezeichnet $\\cdot$ das Skalarprodukt und $\\left| v \\right|$ die Länge des Vektors $v$:\n",
    "$$\n",
    "cos(\\theta) = \\frac{v_1 \\cdot v_2}{\\left|v_1\\right| * \\left|v_2\\right|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c8e1c-cd96-4a6a-bdb1-67a35956b224",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Die Umsetzung in Python ist mit Hilfe von NumPy sehr einfach möglich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede8294-bff8-4a6a-b8bf-efc4aadcdd9c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    np_v1 = np.array(v1)\n",
    "    np_v2 = np.array(v2)\n",
    "\n",
    "    return np.dot(np_v1, np_v2) / (np.linalg.norm(np_v1) * np.linalg.norm(np_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b34a2-71ba-4b17-be36-ce2e481a74cc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Anhand einiger Vektoren lässt sich die Funktion leicht nachvollziehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6de59-072d-4dee-9af3-dd67d46f56c2",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print('unterschiedliche Länge, gleiche Richtung:  ', cos_sim([1, 1], [5, 5]))\n",
    "print('unterschiedliche Länge, ähnliche Richtung: ', cos_sim([1, 1], [2, 1]))\n",
    "print('gleiche Länge, orthogonal:                 ', cos_sim([1, 0], [0, 1]))\n",
    "print('unterschiedliche Länge, fast orthogonal:   ', cos_sim([9, 0], [1, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d0a7a-0062-4d50-a72b-182e4cb67b43",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Zusammenfassung\n",
    "Die Wahl der Repräsentationsform hat starken Einfluss darauf, welche Terme mit welcher Gewichtung in den Dokumentenvektor einfließen. Zur Untersuchung der Ähnlichkeit bietet sich die Cosinus-Ähnlichkeit an, welche die Länge der Vektoren ignoriert und stattdessen den Winkel zwischen diesen zum Vergleich heranzieht."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
