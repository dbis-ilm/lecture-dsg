{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3715e6c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Normalisierung\n",
    "Die Normalisierung hat zum Ziel, zufällige Störungen, die beispielsweise aus Schreibfehlern entstehen, aus dem Text zu entfernen und ihn gleichzeitig näher an eine Art Standardform zu bringen. Diese Standardform reduziert die Varianz, mit der Informationen ausgedrückt werden, und macht somit die Verarbeitung einfacher und zeitgleich effizienter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60abd3b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Inhaltsverzeichnis\n",
    "- [Vorbereitung](#Vorbereitung)\n",
    "- [Groß- und Kleinschreibung](#Groß--und-Kleinschreibung)\n",
    "- [Stopwörter](#Stopwörter)\n",
    "- [Lemmatisierung](#Lemmatisierung)\n",
    "- [Stemming](#Stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eeb60",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Vorbereitung\n",
    "Laden Sie zuerst NLTK und anschließend die Pakete `stopwords` und `wordnet` herunter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554b58c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee76c80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Groß- und Kleinschreibung\n",
    "Im Folgenden werden wir immer wieder Wörter in Kleinbuchstaben umwandeln, um Vergleiche zu vereinfachen. Zeitgleich werden dadurch Fehler in der Groß- und Kleinschreibung ignoriert.\n",
    "\n",
    "Die Python-Funktion `lower` konvertiert jeden Buchstaben einer Zeichenkette in Kleinbuchstaben. Die Funktion `casefold` ist dabei sogar noch etwas aggresiver und konvertiert auch einige Kleinbuchstaben, die außerhalb des lateinsichen Alphabets liegen, in eine Art Stammform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b022b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "text = 'IcH Aß'\n",
    "text.lower(), text.casefold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa13ab",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Da somit auch leichte Schreibfehler automatisch korrigiert werden, ist zu Zwecken der Textanalyse `casefold` vorzuziehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0ce42",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Stopwörter\n",
    "Stopwörter sind solche Wörter, die dem Text kaum Informationen hinzufügen und daher ignoriert werden sollten. In der Regel handelt es sich dabei um Wörter, die so häufig vorkommen, dass sie keiner Bedeutung sinnvoll zugeordnet werden können.\n",
    "\n",
    "Beispiele für diese Wörter sind `in`, `is` / `ist` oder `an` / `ein`. Natürlich bringt NLTK eine Liste solcher Stopwörter für verschiedene Sprachen mit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3ac05",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "list(stopwords.words('english'))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdf07f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "list(stopwords.words('german'))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f670e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Mit Hilfe einer Schleife können Sie nun für einen bereits in Tokens umgewandelten Text prüfen, welche Wörter Stopwörter sind, und diese entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37293a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize('Laue Sommerabende machen mich immer so melancholisch.')\n",
    "\n",
    "filtered_words = []\n",
    "for word in words:\n",
    "    if word not in stopwords.words('german'):\n",
    "        filtered_words.append(word)\n",
    "\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb70d3a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Mit Hilfe einer Comprehension lässt sich dieser Code sogar noch weiter verkürzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd8c10d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "[word for word in words if word.casefold() not in stopwords.words('german')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3409ed",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Lemmatisierung\n",
    "Die Lemmatisierung hat zum Ziel, unterschiedlich flektierte Wortformen als gleiches Wort (Lexem) darzustellen. So soll `geflogen` beispielsweise auf `fliegen` abgebildet werden.\n",
    "\n",
    "Beginnen wir aber zunächst in der englischen Sprache. NLTK stellt mit dem `WordNetLemmatizer` eine einfache Möglichkeit bereit, um Wörter auf ihre Stammform zurückzuführen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3df386",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "WordNetLemmatizer().lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5f2e1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Der Parameter `pos` wird standardmäßig mit `N` wie *Noun* bzw. *Nomen* belegt. Andere Wortarten lassen sich durch Anpassung dieses Parameters auch auf ihre Stammform zurückführen. Legitime Werte sind `V` für Verben, `A` für Adjektive oder `R` für Adverben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd191172",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "WordNetLemmatizer().lemmatize('better', pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca5f3d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Die Lemmatisierung deutscher Wörter unterstützt NLTK leider nicht, sodass eine andere Bibliothek nötig wird. `HanTa` - der Hannover Tagger - implementiert Lemmatisierung für Deutsch, Englisch und Niederländisch. Die folgende Zelle importiert zuerst HanTa, bevor das Modell für die deutsche Sprache geladen wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09e41d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from HanTa import HanoverTagger as ht\n",
    "md = ht.HanoverTagger('morphmodel_ger.pgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac20d1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Die Funktion `analyze` ermöglicht dann die Lemmatisierung, wobei HanTa sogar ein Tupel mit weiteren Informationen zurückgibt.\n",
    "\n",
    "(Der zweite Eintrag im Tupel gibt die Zuordnung zur Wortart an. `NN` steht für Nomen, `VV` für Vollverb und `VA` für Hilfsverb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b996af1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "md.analyze('Hunde'), md.analyze('geflogen'), md.analyze('hätte')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21159392",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Stemming\n",
    "Stemming ist eine Unterform der Lemmatisierung und führt Wörter auf einen künstlichen Wortstamm zurück. Dieser Wortstamm muss selbst kein gültiges Wort der Ausgangssprache sein. NLTK stellt für das Stemming beispielsweise den `SnowballStemmer` bereit, der sowohl mit englischen wie auch deutschen Wörtern interagieren kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae5091",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "snowball.stem('care'), snowball.stem('cared'), snowball.stem('caring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f161a5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('german')\n",
    "\n",
    "snowball.stem('besuchen'), snowball.stem('besuche'), snowball.stem('besuchte')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
